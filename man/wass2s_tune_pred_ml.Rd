% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hydro-tune.R
\name{wass2s_tune_pred_ml}
\alias{wass2s_tune_pred_ml}
\title{Tune and predict for one product and one ML model}
\usage{
wass2s_tune_pred_ml(
  df_basin_product,
  predictors,
  target = "Q",
  date_col = "YYYY",
  prediction_years = NULL,
  model = SUPPORTED_MODELS,
  resamples = NULL,
  grid_levels = 5,
  seed = 123,
  pretrained_wflow = NULL,
  init_frac = 0.6,
  assess_frac = 0.2,
  n_splits = 3,
  cumulative = TRUE,
  quiet = TRUE,
  target_positive = TRUE,
  allow_par = TRUE,
  ...
)
}
\arguments{
\item{df_basin_product}{A data frame containing at least the columns
\code{YYYY}, \code{Q}, and predictor variables.}

\item{predictors}{Character vector of predictor names to use.}

\item{target}{Name of the target column (default: `"Q"`).}

\item{date_col}{Name of the date column (default: `"YYYY"`).}

\item{prediction_years}{Optional numeric vector of length 2 giving the
start and end years for a holdout prediction period. These years
are excluded from training and predictions are generated after fitting.}

\item{model}{One of \code{SUPPORTED_MODELS}, e.g. `"rf"`, `"xgb"`, `"mlp"`.}

\item{resamples}{Optional \code{rsample::rset} object for resampling.
If \code{NULL}, a rolling-origin resampling is created via
\code{make_rolling()}.}

\item{grid_levels}{Number of grid levels per parameter (default: 5).}

\item{seed}{Random seed for reproducibility.}

\item{pretrained_wflow}{Optional \code{workflows::workflow} object.
If supplied, tuning is skipped and the workflow is fitted directly.}

\item{init_frac}{Fraction of rows used for the initial training window
(default 0.60). A hard minimum of 8 rows is enforced when possible.}

\item{assess_frac}{Fraction of rows used for the assessment window
(default 0.20). A hard minimum of 3 rows is enforced when possible.}

\item{n_splits}{Optional integer, desired number of resamples (splits).
If \code{NULL} (default), every possible split is produced (\code{skip = 0}).}

\item{cumulative}{Logical; passed to \code{rsample::rolling_origin()}
(default \code{TRUE}).}

\item{quiet}{Logical; if \code{FALSE}, emits informative messages when the
requested \code{n_splits} cannot be reached (default \code{TRUE}).}

\item{target_positive}{Logical; if TRUE, force negative predictions to zero.}

\item{allow_par}{A logical to allow parallel processing (if a parallel backend is registered).}

\item{...}{Others parameters passed to \code{tune::control_grid()}}
}
\value{
A list with the following elements:
\itemize{
  \item \code{kge_cv_mean} Mean KGE of the best configuration across CV splits.
  \item \code{preds} Tibble with columns \code{YYYY, pred} containing
    predictions from the final fit (training + optional holdout).
  \item \code{fit} Final fitted workflow object.
  \item \code{leaderboard_cfg} Tibble of model configurations ranked by KGE.
   \item \code{param_grid} Tibble of parameters used for model training.
}
}
\description{
This function performs cross-validation tuning for a single
(basin, product) dataset using a specified machine learning model.
It ranks model configurations by KGE (computed from CV predictions),
refits the best configuration on the full dataset, and returns fitted
predictions for both training and optional holdout (prediction) years.
}
\details{
- The target column is always standardized internally to \code{Q}
  and the date column to \code{YYYY} before modeling.
- If \code{prediction_years} is given, those years are removed from
  the training data and used as an out-of-sample prediction set.
- Model configurations are tuned using RMSE as the optimization metric
  but are ranked by KGE for reporting.
}
\examples{
\dontrun{
# Example with toy data
df <- tibble::tibble(
  YYYY = 1990:2000,
  Q = rnorm(11, 1000, 200),
  x1 = rnorm(11), x2 = rnorm(11)
)

res <- wass2s_tune_pred_ml(
  df_basin_product = df,
  predictors = c("x1", "x2"),
  model = "rf",
  grid_levels = 3
)

res$kge_cv_mean
res$preds
}

}
